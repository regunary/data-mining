version: "3.9"

services:
  # Dremio Service
  dremio:
    platform: linux/x86_64
    image: dremio/dremio-oss:latest
    environment:
      - DREMIO_ADMIN_USER=admin
      - DREMIO_ADMIN_PASSWORD=admin
      - DREMIO_DATA_DIR=/var/lib/dremio
    ports:
      - 9047:9047 # Dremio UI
      - 31010:31010 # Dremio ODBC/JDBC
      - 32010:32010
    container_name: dremio

  # MinIO Service
  minioserver:
    image: minio/minio
    ports:
      - 9000:9000 # minio console
      - 9001:9001 # minio admin console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    container_name: minioserver
    command: server /data --console-address ":9001"
  
  # Nessie Service
  nessie:
    image: projectnessie/nessie
    container_name: nessie
    ports:
      - "19120:19120"

  # Airflow Service
  postgres:
    container_name: postgres
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.7.0
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      PYTHONPATH: /opt/airflow
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    command: >
        bash -c "airflow db init && airflow db migrate &&
           airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin"

    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow:/opt/airflow
    healthcheck:
      test: ["CMD-SHELL", "airflow db check || exit 1"]
      interval: 10s
      retries: 5
      start_period: 5s
      timeout: 5s

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.7.0
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      PYTHONPATH: /opt/airflow
      DJANGO_SETTINGS_MODULE: src.configs.settings
      AIRFLOW__WEBSERVER__DEFAULT_UI_USER_ROLE: Admin
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__API__ENABLE_EXPERIMENTAL_API: 'True'
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      AIRFLOW_WEBSERVER_BASE_URL: http://localhost:8080
      AIRFLOW__CORE__FERNET_KEY: "QeC9m8DBa9Pmq5yztQ80yU8Y5Xo0gx79gCXx4i9CP_g="
    command: bash -c "airflow webserver"
    ports:
      - "8088:8080"  # Expose UI on port 8088
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow:/opt/airflow
      - ./requirements.txt:/opt/airflow/requirements.txt
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 120s
      timeout: 20s
      retries: 5
      start_period: 90s
    # networks:
    #   - default

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.7.0
    container_name: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      PYTHONPATH: /opt/airflow
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW_WEBSERVER_BASE_URL: http://localhost:8080
      AIRFLOW__CORE__FERNET_KEY: "QeC9m8DBa9Pmq5yztQ80yU8Y5Xo0gx79gCXx4i9CP_g="
    command: >
      bash -c "airflow scheduler"
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow:/opt/airflow
      - ./requirements.txt:/opt/airflow/requirements.txt
    networks:
      - default

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - default

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    hostname: schema-registry
    depends_on:
      - kafka-broker-1
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka-broker-1:9092"
      SCHEMA_REGISTRY_DEBUG: 'true'
    networks:
      - default

  kafka-broker-1:
    image: confluentinc/cp-kafka:latest
    hostname: kafka-broker-1
    ports:
      - "19092:19092"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_MESSAGE_MAX_BYTES: 2097152000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 2097152000
    healthcheck:
      test: ["CMD", "sh", "-c", "echo 'disconnected' | nc -w 5 localhost 9092 || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 5
    networks:
      - default

  control-center:
    image: confluentinc/cp-enterprise-control-center:7.5.1
    hostname: control-center
    container_name: control-center
    depends_on:
      - kafka-broker-1
      - schema-registry
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'kafka-broker-1:9092'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      CONFLUENT_METRICS_ENABLE: 'false'
      PORT: 9021
    networks:
      - default
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9021/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
    
  kafka-connect:
    build: 
      context: .
      dockerfile: Dockerfile_kafka_connect
    image: confluentinc/cp-kafka-connect:latest
    ports:
      - "8083:8083"
    depends_on:
      kafka-broker-1:
        condition: service_healthy
    environment:
      BOOTSTRAP_SERVERS: "PLAINTEXT_INTERNAL://kafka-broker-1:9092"
      GROUP_ID: compose-connect-group
      REST_HOST: kafka-connect
      CONFIG_STORAGE_TOPIC: connect-configs
      OFFSET_STORAGE_TOPIC: connect-offsets
      STATUS_STORAGE_TOPIC: connect-status
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_BOOTSTRAP_SERVERS: "PLAINTEXT_INTERNAL://kafka-broker-1:9092"
      CONNECT_REST_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: All
      CONNECT_PLUGIN_PATH: "/opt/kafka/plugins"
    volumes:
      - ./data:/data
    networks:
      - default

volumes:
  postgres_data:
    driver: local
  airflow-config:
    driver: local

networks:
  default:
    driver: bridge
